"""
Create function for loading phothos
Tokenizing the vocabulary
Saving tokenized vocabulary

"""

import json
import pickle
from pickle import dump

from tensorflow import keras

from clean_descriptions import read_captions


def load_photos(filename):
    """Load predefined list of photos.
    Can be used to load train and test set
    """
    file = open(filename, 'r')
    text = file.read()
    file.close()
    file2 = text
    photos = file2.split("\n")[:-1]
    return photos


def load_clean_descriptions(path, dataset):
    """
    Loads cleaned descriptions(captions) for predefined images.
    Embbeds each description in startseq and endseq. Needed for LSTM.
    ### Parameters:
        path: path to the clean descriptions dictionary
        dataset: names of the images for which we wnat to grab the description (generated by load_phothos)
    ### Returns:
        dictionary: keys=image names, values=list of cleaned descriptions embedded in startseq and endseq.

    """
    with open(path) as f:
        doc = json.load(f)
    descriptions = dict()
    for key, value in doc.items():

        image_id, image_desc = key, value
        # skip images not in the set
        for vals in image_desc:
            if image_id in dataset:
                # create list
                if image_id not in descriptions:
                    descriptions[image_id] = list()
                # wrap description in tokens
                desc = 'startseq ' + str(vals) + ' endseq'
                # store
                descriptions[image_id].append(desc)
    return descriptions

# covert a dictionary of clean descriptions to a list of descriptions


def to_lines(descriptions):
    """Splits the list of descriptions for each image.
        Needed for tokenizer.
    """
    all_desc = list()
    for key in descriptions.keys():
        [all_desc.append(d) for d in descriptions[key]]
    return all_desc

# fit a tokenizer given caption descriptions


def create_tokenizer(descriptions):
    """Tokenizes the vocabulary.
    """
    lines = to_lines(descriptions)
    tokenizer = keras.preprocessing.text.Tokenizer()
    tokenizer.fit_on_texts(lines)
    return tokenizer


def max_seq_lenght(description):
    """Calculates the caption length.
    Needed for model input layer.
    """
    lines = to_lines(description)
    maximum = max(len(d.split()) for d in lines)
    return maximum


if __name__ == "__main__":

    # load training dataset (6K)
    filename = './Flickr8k_text/Flickr_8k.trainImages.txt'
    train = load_photos(filename)
    print('Dataset: %d' % len(train))
    # descriptions
    train_descriptions = load_clean_descriptions(
        './model_files/description.json', train)
    print('Descriptions: train=%d' % len(train_descriptions))
    pickle.dump(train_descriptions, open(
        './model_files/train_descriptions.pkl', 'wb'))

    # prepare tokenizer
    tokenizer = create_tokenizer(train_descriptions)
    # save the tokenizer
    pickle.dump(tokenizer, open('./model_files/tokenizer.pkl', 'wb'))
    vocab_size = len(tokenizer.word_index) + 1
    print(vocab_size)

    # max length of the sequence
    print("max length:")
    print(max_seq_lenght(train_descriptions))
